# RAG QA System

## Assignment Objective

Build a **production-ready Retrieval-Augmented Generation (RAG)** application that answers questions strictly from the Swiggy Annual Report PDF â€” with zero hallucination, fast retrieval, and a clean Streamlit UI.

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     STREAMLIT UI (app.py)                   â”‚
â”‚  Sidebar: Upload PDF â†’ Process Button â†’ Status Messages     â”‚
â”‚  Main:    Question Input â†’ Get Answer â†’ Answer + Context    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               DOCUMENT PROCESSING (rag_pipeline.py)         â”‚
â”‚                                                             â”‚
â”‚  PyPDFLoader (lazy_load - page by page)                     â”‚
â”‚       â†“                                                     â”‚
â”‚  Text Cleaning (regex - whitespace, hyphenation)            â”‚
â”‚       â†“                                                     â”‚
â”‚  RecursiveCharacterTextSplitter                             â”‚
â”‚  chunk_size=1200, chunk_overlap=200                         â”‚
â”‚  + Metadata: page number, source filename                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               EMBEDDING + VECTOR STORE                      â”‚
â”‚                                                             â”‚
â”‚  OpenAIEmbeddings (text-embedding-3-large)                  â”‚
â”‚  Batched: 64 chunks/batch â†’ memory safe                     â”‚
â”‚       â†“                                                     â”‚
â”‚  FAISS Index (built + saved locally to ./faiss_index/)      â”‚
â”‚  On rerun â†’ load existing index (no re-embedding)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               ADVANCED RETRIEVAL PIPELINE                   â”‚
â”‚                                                             â”‚
â”‚  Stage 1: FAISS similarity_search â†’ Top 10 chunks          â”‚
â”‚       â†“                                                     â”‚
â”‚  Stage 2: CrossEncoder re-ranking                           â”‚
â”‚           (ms-marco-MiniLM-L-6-v2) â†’ Top 5                 â”‚
â”‚       â†“                                                     â”‚
â”‚  Stage 3: Final Top 3 chunks â†’ passed to LLM               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               LLM ANSWER GENERATION                         â”‚
â”‚                                                             â”‚
â”‚  GPT-4o (temperature=0, streaming)                          â”‚
â”‚  Anti-hallucination system prompt                           â”‚
â”‚  Cites page numbers in answer                               â”‚
â”‚  Falls back safely if answer not in context                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Project Structure

```
rag_app/
â”‚
â”œâ”€â”€ app.py              â† Streamlit UI
â”œâ”€â”€ rag_pipeline.py     â† Core RAG logic
â”œâ”€â”€ requirements.txt    â† Python dependencies
â”œâ”€â”€ .env.example        â† Environment variable template
â””â”€â”€ README.md           â† This file

faiss_index/            â† Auto-created after first processing
â”œâ”€â”€ index.faiss
â””â”€â”€ index.pkl
```

---

## Setup Steps

### 1. Clone / Download the project

```bash
git clone <https://github.com/saurabhbhandariii/swiggy_rag>
cd rag_app
```

### 2. Create a virtual environment

```bash
python3.10 -m venv venv
source venv/bin/activate        # Windows: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure environment variables

```bash
cp .env.example .env
# Edit .env and add your OpenAI API key
nano .env
```

### 5. Obtain the Swiggy Annual Report PDF

Download the Swiggy Annual Report from:
> ğŸ“ **Source:** [Swiggy Investor Relations â€“ Annual Reports](https://ir.swiggy.in/financial-information/annual-reports)  
> *(Download the latest available Annual Report PDF)*

---

## How to Run

```bash
streamlit run app.py
```

Open your browser at **http://localhost:8501**

### Usage Flow

1. **Upload PDF** via the sidebar file uploader
2. Click **"âš™ï¸ Process Document"** â€” chunks, embeds, and saves the FAISS index
3. **Type a question** in the main area text box
4. Click **"ğŸ” Get Answer"**
5. View the **answer**, **supporting context chunks**, and **page references**

> âœ… On subsequent runs, the FAISS index is auto-loaded from disk â€” no re-embedding needed.

---

## Key Design Decisions

### Chunking Strategy

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| `chunk_size` | 1200 chars | Balances context richness vs. retrieval precision |
| `chunk_overlap` | 200 chars | Prevents context loss at chunk boundaries |
| Splitter | `RecursiveCharacterTextSplitter` | Respects paragraph/sentence structure |
| Separators | `\n\n`, `\n`, `. `, ` `, `""` | Hierarchical splitting for clean boundaries |

The PDF is loaded page-by-page using `PyPDFLoader.lazy_load()` to prevent memory exhaustion on 200+ page documents. Each chunk retains its source page number as metadata.

---

### Retrieval Strategy

A **3-stage pipeline** is used to maximize precision:

```
FAISS Similarity Search (Top 10)
         â†“
Cross-Encoder Re-Ranking (Top 5)
         â†“
Top 3 â†’ LLM
```

**Stage 1 â€“ Dense Retrieval (FAISS):**  
Fast approximate nearest-neighbor search using cosine similarity on `text-embedding-3-large` vectors. Returns the top-10 most semantically similar chunks.

**Stage 2 â€“ Cross-Encoder Re-Ranking:**  
The `ms-marco-MiniLM-L-6-v2` cross-encoder scores each (query, chunk) pair jointly â€” unlike bi-encoders, it reads both together for much higher precision. The top-5 are kept.

**Stage 3 â€“ LLM Context:**  
Only the final top-3 chunks are passed to GPT-4o, keeping the prompt focused and within token budget.

---

### Anti-Hallucination Method

Three complementary layers prevent hallucination:

1. **Strict System Prompt:**  
   The LLM is explicitly instructed to answer only from provided context and respond with a standard message (`"The answer is not available in the provided document."`) if the answer isn't present.

2. **Temperature = 0:**  
   Deterministic generation eliminates creative guessing.

3. **Page Citation Requirement:**  
   The prompt mandates citing page numbers, forcing the model to ground its answer in specific document locations.

4. **Context-Only Architecture:**  
   No web search, no external tools, no model fine-tuning â€” the LLM only sees the retrieved chunks.

---

## Environment Variables

| Variable | Description |
|----------|-------------|
  | `OPENAI_GROQ_KEY` | Your groq API key (required) |

---

## Performance Notes

- **Batched embedding** (64 chunks/batch) prevents OOM errors on large PDFs
- **FAISS persistence** eliminates re-embedding on subsequent runs
- **`@st.cache_resource`** caches the index load across Streamlit reruns
- **`st.session_state`** tracks processing status and prevents duplicate work
- **Lazy PDF loading** (`lazy_load()`) streams pages without loading the full file into RAM

  demo--
  
